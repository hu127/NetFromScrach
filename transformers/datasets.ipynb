{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# datasets from huggingface\n",
    "from datasets import load_dataset # huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 32332\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# get datasets\n",
    "dataset_raw  = load_dataset('opus_books', 'en-it') # have to specify language pair\n",
    "print(dataset_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'translation': {'en': 'Source: Project Gutenberg', 'it': 'Source: www.liberliber.it/Audiobook available here'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_raw['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'translation'],\n",
      "    num_rows: 32332\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_split = load_dataset('opus_books', 'en-it', split='train')\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'translation': {'en': 'Source: Project Gutenberg', 'it': 'Source: www.liberliber.it/Audiobook available here'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'translation': {'en': 'Jane Eyre', 'it': 'Jane Eyre'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer # tokenizers\n",
    "from tokenizers.models import WordLevel # tokenizers models\n",
    "from tokenizers.trainers import WordLevelTrainer # tokenizers trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace # tokenizers pre-tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\")) # initialize tokenizer with WordLevel model\n",
    "tokenizer.pre_tokenizer = Whitespace() # set pre-tokenizer to Whitespace\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[EOS]\", \"[SOS]\"]) # initialize trainer with special tokens\n",
    "# special tokens\n",
    "# [UNK] - unknown token\n",
    "# [CLS] - classification token\n",
    "# [SEP] - separator token\n",
    "# [PAD] - padding token\n",
    "# [MASK] - mask token\n",
    "# [SOS] - start of sentence token\n",
    "# [EOS] - end of sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(dataset, lang):\n",
    "    for item in dataset:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Project Gutenberg\n",
      "Jane Eyre\n"
     ]
    }
   ],
   "source": [
    "# generate iterator for all sentences\n",
    "sentences = get_all_sentences(dataset_split, 'en')\n",
    "print(next(sentences))\n",
    "print(next(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(sentences, trainer=trainer) # train tokenizer from iterator\n",
    "tokenizer.save(\"en-it-wordlevel.json\") # save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# delete en-it-wordlevel.json\n",
    "if sys.platform == \"linux\":\n",
    "    !rm en-it-wordlevel.json\n",
    "elif sys.platform == \"win32\":\n",
    "    !del en-it-wordlevel.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260, 37, 14, 8498, 1888, 10]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"This is a test sentence.\"\n",
    "encoded = tokenizer.encode(test_sentence) # encode sentence\n",
    "print(encoded.ids) # print encoded sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 17991, 32, 0, 0, 1857, 0, 17464, 17405, 32, 0, 0, 29, 0, 7, 8767, 7, 10294]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Morning Seaside Coffe Shop - Relaxing Jazz & Bossa Nova Music - Piano Jazz for Studying, Sleep, Work\"\n",
    "encoded = tokenizer.encode(test_sentence) # encode sentence\n",
    "print(encoded.ids) # print encoded sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
